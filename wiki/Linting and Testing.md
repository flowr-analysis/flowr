_This document was generated from '[src/documentation/print-linting-and-testing-wiki.ts](https://github.com/flowr-analysis/flowr/tree/main//src/documentation/print-linting-and-testing-wiki.ts)' on 2025-08-06, 20:22:56 UTC presenting an overview of flowR's linting and testing definitions (v2.3.0). Please do not edit this file/wiki page directly._

For the latest code coverage information, see [codecov.io](https://app.codecov.io/gh/flowr-analysis/flowr), 
for the latest benchmark results, see the [benchmark results](https://flowr-analysis.github.io/flowr/wiki/stats/benchmark) wiki page.

- [üè® Testing Suites](#testing-suites)
  - [üß™ Functionality Tests](#functionality-tests)
    - [üèóÔ∏è Test Structure](#test-structure)
    - [üè∑Ô∏è Test Labels](#test-labels)
    - [üñãÔ∏è Writing a Test](#writing-a-test)
    - [ü§è Running Only Some Tests](#running-only-some-tests)
  - [üíΩ System Tests](#system-tests)
  - [üíÉ Performance Tests](#performance-tests)
  - [üìù Testing Within Your IDE](#testing-within-your-ide)
    - [VS Code](#vs-code)
    - [Webstorm](#webstorm)
- [ü™à CI Pipeline](#ci-pipeline)
- [üßπ Linting](#linting)
  - [Oh no, the linter fails](#oh-no-the-linter-fails)
  - [License Checker](#license-checker)
- [üêõ Debugging](#debugging)
  - [VS Code](#vs-code-1)
  - [Logging](#logging)

<a id='testing-suites'></a>
## üè® Testing Suites

Currently, flowR contains three testing suites: one for [functionality](#functionality-tests), 
one for [system tests](#system-tests), and one for [performance](#performance-tests). We explain each of them in the following.
In addition to running those tests, you can use the more generalized `npm run checkup`. 
This command includes the construction of the docker image, the generation of the wiki pages, and the linter.

<a id='functionality-tests'></a>
### üß™ Functionality Tests

The functionality tests represent conventional unit (and depending on your terminology component/api) tests.
We use [vitest](https://vitest.dev/) as our testing framework.
You can run the tests by issuing (some quick benchmarks may be available with `vitest bench`):


```shell
npm run test
```


Within the commandline,
this should automatically drop you into a watch mode which will automatically re-run (potentially) affected tests if you change the code.
If, at any time there are too many errors for you to comprehend, you can use `--bail=<value>` to stop the tests after a certain number of errors.
For example:


```shell
npm run test -- --bail=1
```


If you want to run the tests without the watch mode, you can use:


```shell
npm run test -- --no-watch
```


To run all tests, including a coverage report and label summary, run:


```shell
npm run test-full
```


However, depending on your local version of&nbsp;R, your network connection, and other factors (each test may have a set of criteria), 
some tests may be skipped automatically as they do not apply to your current system setup (or cannot be tested with the current prerequisites). 
Each test can specify such requirements as part of the `TestConfiguration`, which is then used in the `test.skipIf` function of _vitest_.
It is up to the [ci](#ci-pipeline) to run the tests on different systems to ensure that those tests run.

<a id='test-structure'></a>
#### üèóÔ∏è Test Structure

All functionality tests are to be located under [test/functionality](https://github.com/flowr-analysis/flowr/tree/main//test/functionality).

This folder contains three special and important elements:

- `test-setup.ts` which is the entry point if *all* tests are run. It should automatically disable logging statements and configure global variables (e.g., if installation tests should run).
- `_helper/` folder which contains helper functions to be used by other tests.
- `test-summary.ts` which may produce a summary of the covered capabilities.


> [!WARNING]
> 
> We name all test files using the `.test.ts` suffix and try to run them in parallel.
> Whenever this is impossible (e.g., when using <a href="https://github.com/flowr-analysis/flowr/tree/main//test/functionality/_helper/shell.ts#L89"><code><span title="Produces a shell session for you, can be used within a describe block. Please use **describe.sequential** as the RShell does not fare well with parallelization.">withShell</span></code></a>), please use _`describe.sequential`_
> to disable parallel execution for the respective test (otherwise, such tests are flaky).
> 


<a id='test-labels'></a>
#### üè∑Ô∏è Test Labels

Generally, tests are [labeled](https://github.com/flowr-analysis/flowr/tree/main/test/functionality/_helper/label.ts) according to the *flowR* capabilities they test. 

The set of currently supported capabilities and their IDs can be found in [`./src/r-bridge/data/data.ts`](https://github.com/flowr-analysis/flowr/tree/main/./src/r-bridge/data/data.ts). 

The resulting labels are used in the test report that is generated as part of the test output. 
They group tests by the capabilities they test and allow the report to display how many tests ensure that any given capability is properly supported.
The report can be found on the wiki's [capabilities page](https://github.com/flowr-analysis/flowr/wiki/Capabilities).

To add new labels, simply add them to the relevant section in [`./src/r-bridge/data/data.ts`](https://github.com/flowr-analysis/flowr/tree/main/./src/r-bridge/data/data.ts) as part of a pull request.

<a id='writing-a-test'></a>
#### üñãÔ∏è Writing a Test

Currently, this is heavily dependent on what you want to test (normalization, dataflow, quad-export, ‚Ä¶) 
and it is probably best to have a look at existing tests in that area to get an idea of what comfort functionality is available.

Various helper functions are available to ease in writing tests with common behaviors, like testing for dataflow, slicing or query results. 
These can be found in [the `_helper` subdirectory](https://github.com/flowr-analysis/flowr/tree/main/test/functionality/_helper).

For example, an [existing test](https://github.com/flowr-analysis/flowr/tree/main/test/functionality/dataflow/processing-of-elements/atomic/dataflow-atomic.test.ts) that tests the dataflow graph of a simple variable looks like this:

```typescript
assertDataflow(label('simple variable', ['name-normal']), shell,
	'x', emptyGraph().use('0', 'x')
);
```

Have a look at <a href="https://github.com/flowr-analysis/flowr/tree/main//test/functionality/_helper/shell.ts#L363"><code><span title="Your best friend whenever you want to test whether the dataflow graph produced by flowR is as expected.  You may want to have a look at the DataflowTestConfiguration to see what you can configure. Especially the resolveIdsAsCriterion and the expectIsSubgraph are interesting as they allow you for rather flexible matching of the expected graph.">assertDataflow</span></code></a>, <a href="https://github.com/flowr-analysis/flowr/tree/main//test/functionality/_helper/label.ts#L51"><code><span title="Wraps a test name with a unique identifier and label it with the given ids. Test labels are used for identifying which of flowR's capabilities are being tested for. For more information about test labels and capabilities, see the wiki page: https://github.com/flowr-analysis/flowr/wiki/Linting-and-Testing#test-labels">label</span></code></a>, and <a href="https://github.com/flowr-analysis/flowr/tree/main//src/dataflow/graph/dataflowgraph-builder.ts#L23"><code>emptyGraph</code></a> for more information.

When writing dataflow tests, additional settings can be used to reduce the amount of graph data that needs to be pre-written. Notably:

- <a href="https://github.com/flowr-analysis/flowr/tree/main//test/functionality/_helper/shell.ts#L341"><code><span title="Specify just a subset of what the dataflow graph will actually be.">expectIsSubgraph</span></code></a> indicates that the expected graph is a subgraph, rather than the full graph that the test should generate. 
  The test will then only check if the supplied graph is contained in the result graph, rather than an exact match.
- <a href="https://github.com/flowr-analysis/flowr/tree/main//test/functionality/_helper/shell.ts#L349"><code><span title="This changes the way the test treats the NodeId s in your expected graph. Before running the verification, the test environment will transform the graph, resolving all Ids as if they are slicing criteria. In other words, you can use the criteria 12@a which will be resolved to the corresponding id before comparing. Please be aware that this is currently a work in progress.">resolveIdsAsCriterion</span></code></a> indicates that the ids given in the expected (sub)graph should be resolved as [slicing criteria](https://github.com/flowr-analysis/flowr/wiki/Terminology#slicing-criterion) rather than actual ids. 
  For example, passing `12@a` as an id in the expected (sub)graph will cause it to be resolved as the corresponding id.

The following example shows both in use:

```typescript
assertDataflow(label('without distractors', [...OperatorDatabase['<-'].capabilities, 'numbers', 'name-normal', 'newlines', 'name-escaped']),
	shell, '`a` <- 2\na',
	emptyGraph()
		.use('2@a')
		.reads('2@a', '1@`a`'),
	{
		expectIsSubgraph:      true,
		resolveIdsAsCriterion: true
	}
);
```


<a id='running-only-some-tests'></a>
#### ü§è Running Only Some Tests

To run only some tests, vitest allows you to [filter](https://vitest.dev/guide/filtering.html) tests. 
Besides, you can use the watch mode (with `npm run test`) to only run tests that are affected by your changes.

<a id='system-tests'></a>
### üíΩ System Tests

In contrast to the [functionality tests](#functionality-tests), the system tests use runners like the `npm` scripts
to test the behavior of the whole system, for example, by running the CLI or the server.
They are slower and hence not part of `npm run test` but can be run using:

```shell
npm run test:system
```

To work, they require you to set up your system correctly (e.g., have `npm` available on your path).
The CI environment will make sure of that. At the moment, these tests are not labeled and only intended
to check basic availability of *flowR*'s core features (as we test the functionality of these features dedicately 
with the [functionality tests](#functionality-tests)).

Have a look at the [test/system-tests](https://github.com/flowr-analysis/flowr/tree/main/test/system-tests) folder for more information.
 
<a id='performance-tests'></a>
### üíÉ Performance Tests

The performance test suite of *flowR* uses several suites to check for variations in the required times for certain steps.
Although we measure wall time in the CI (which is subject to rather large variations), it should give a rough idea *flowR*'s performance.
Furthermore, the respective scripts can be used locally as well.
To run them, issue:


```shell
npm run performance-test
```


See [test/performance](https://github.com/flowr-analysis/flowr/tree/main/test/performance) for more information on the suites, how to run them, and their results. If you are interested in the results of the benchmarks, see [here](https://flowr-analysis.github.io/flowr/wiki/stats/benchmark).

<a id='testing-within-your-ide'></a>
### üìù Testing Within Your IDE

#### VS Code

Using the vitest Extension for Visual Studio Code, you can start tests directly from the definition and explore your suite in the Testing tab.
To get started, install the [vitest Extension](https://marketplace.visualstudio.com/items?itemName=vitest.explorer).

|               Testing Tab               | In Code                               |
|:---------------------------------------:|:-------------------------------------:|
| ![testing tab](img/testing-vs-code.png) | ![in code](img/testing-vs-code-2.png) |

- Left-clicking the <img style="vertical-align: middle" src='img/circle-check-regular.svg' height='16pt'> or <img style="vertical-align: middle" src='img/circle-xmark-regular.svg' height='16pt'> Icon next to the code will rerun the test. Right-clicking will open a context menu, allowing you to debug the test.
- In the Testing tab, you can run (and debug) all tests, individual suites or individual tests.

#### Webstorm

Please follow the official guide [here](https://www.jetbrains.com/help/webstorm/vitest.html).

<a id='ci-pipeline'></a>
## ü™à CI Pipeline

We have several workflows defined in [.github/workflows](https://github.com/flowr-analysis/flowr/tree/main//.github/workflows/).
We explain the most important workflows in the following:

- [qa.yaml](https://github.com/flowr-analysis/flowr/tree/main//.github/workflows/qa.yaml) is the main workflow that will run different steps depending on several factors. It is responsible for:
  - running the [functionality](#functionality-tests) and [performance tests](#performance-tests)
    - uploading the results to the [benchmark page](https://flowr-analysis.github.io/flowr/wiki/stats/benchmark) for releases
    - running the [functionality tests](#functionality-tests) on different operating systems (Windows, macOS, Linux) and with different versions of R
    - reporting code coverage
  - running the [linter](#linting) and reporting its results
  - deploying the documentation to [GitHub Pages](https://flowr-analysis.github.io/flowr/doc/)
- [release.yaml](https://github.com/flowr-analysis/flowr/tree/main//.github/workflows/release.yaml) is responsible for creating a new release, only to be run by repository owners. Furthermore, it adds the new docker image to [docker hub](https://hub.docker.com/r/eagleoutice/flowr).
- [broken-links-and-wiki.yaml](https://github.com/flowr-analysis/flowr/tree/main//.github/workflows/broken-links-and-wiki.yaml) repeatedly tests that all links are not dead!
 
<a id='linting'></a>
## üßπ Linting

There are two linting scripts.
The main one:


```shell
npm run lint
```


And a weaker version of the first (allowing for *todo* comments) which is run automatically in the [pre-push githook](https://github.com/flowr-analysis/flowr/tree/main//.githooks/pre-push) as explained in the [CONTRIBUTING.md](https://github.com/flowr-analysis/flowr/tree/main//.github/CONTRIBUTING.md):


```shell
npm run lint-local
```


Besides checking coding style (as defined in the [package.json](https://github.com/flowr-analysis/flowr/tree/main//package.json)), the *full* linter runs the [license checker](#license-checker).

In case you are unaware,
eslint can automatically fix several linting problems[](https://eslint.org/docs/latest/use/command-line-interface#fix-problems).
So you may be fine by just running:


```shell
npm run lint-local -- --fix
```

 
<a id='oh-no-the-linter-fails'></a>
### üí• Oh no, the linter fails

By now, the rules should be rather stable and so, if the linter fails,
it is usually best if you (when necessary) read the respective description and fix the respective problem.
Rules in this project cover general JavaScript issues [using regular ESLint](https://eslint.org/docs/latest/rules), TypeScript-specific issues [using typescript-eslint](https://typescript-eslint.io/rules/), and code formatting [with ESLint Stylistic](https://eslint.style/packages/default#rules).

However, in case you think that the linter is wrong, please do not hesitate to open a [new issue](https://github.com/flowr-analysis/flowr/issues/new/choose).
 
<a id='license-checker'></a>
### ü™™ License Checker

*flowR* is licensed under the [GPLv3 License](https://github.com/flowr-analysis/flowr/blob/main/LICENSE) requiring us to only rely on [compatible licenses](https://www.gnu.org/licenses/license-list.en.html). For now, this list is hardcoded as part of the npm [`license-compat`](https://github.com/flowr-analysis/flowr/tree/main//package.json) script so it can very well be that a new dependency you add causes the checker to fail &mdash; *even though it is compatible*. In that case, please either open a [new issue](https://github.com/flowr-analysis/flowr/issues/new/choose) or directly add the license to the list (including a reference to why it is compatible).

<a id='debugging'></a>
## üêõ Debugging
### VS Code
When working with VS Code, you can attach a debugger to the REPL. This works automatically by running the `Start Debugging` command (`F5` by default).
You can also set the `Auto Attach Filter` setting to automatically attach the debugger, when running `npm run flowr`.

### Logging

*flowR* uses a wrapper around [tslog](https://www.npmjs.com/package/tslog) using a class named
<a href="https://github.com/flowr-analysis/flowr/tree/main//src/util/log.ts#L10"><code>FlowrLogger</code></a>. They obey to, for example, the <span title="Description (Command Line Argument): Run with verbose logging (will be passed to the corresponding script)">`--verbose`</span>
option. Throughout *flowR*, we use the `log` object (or subloggers of it) for logging.
To create your own logger, you can use <a href="https://github.com/flowr-analysis/flowr/tree/main//src/util/log.ts#L14"><code>getSubLogger</code></a>.
For example, check out the <a href="https://github.com/flowr-analysis/flowr/tree/main//src/slicing/static/static-slicer.ts#L20"><code>slicerLogger</code></a> for the static slicer.


